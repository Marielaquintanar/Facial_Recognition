# -*- coding: utf-8 -*-
"""Facial_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_qGt0bYEphwNs-orRSQrlH03t9bXE2HV

**FACIAL RECOGNITION**
"""

import tensorflow as tf # mobile net system
from keras.models import load_model # to load facenet
import cv2 # allows us to read images from the hard drive and store them
import numpy as np # to make calculations on faces that are detected
import matplotlib.pyplot as plt # to draw the images that we obtained
import os # to be able to read the data on the hard drive
!pip install keras_facenet
from keras_facenet import FaceNet
from tensorflow.keras.models import load_model

# Folders to store the known and unknown images and the results
DIR_KNOWNS = 'knowns'
DIR_UNKNOWNS = 'unknowns'
DIR_RESULTS = 'results'

# read mobilenet_graph.pb
with tf.io.gfile.GFile('frozen_inference_graph_face.pb','rb') as f:
    graph_def = tf.compat.v1.GraphDef()
    graph_def.ParseFromString(f.read())

with tf.Graph().as_default() as mobilenet:
    tf.import_graph_def(graph_def,name='')

print(mobilenet)

# DIR -> directorio
# NAME -> nombre del archivo
def load_image(DIR, NAME):
    return cv2.cvtColor(cv2.imread(f'{DIR}/{name}'), cv2.COLOR_BGR2RGB)

# bounding boxes from 0.7 up
def detect_faces(image, score_threshold=0.7):
    global boxes, scores
    (imh, imw) = image.shape[:-1]
    img = np.expand_dims(image,axis=0)

  # at first our image only has 3 dimensions but since
  # we want to add several images at the same time we must create another dimension

  # initialize mobilenet
  # with 3 containers
  # container 1: we indicate where the input image is stored
  # container 2 and 3: output containers (bounding boxes)

    sess = tf.compat.v1.Session(graph=mobilenet)
    image_tensor = mobilenet.get_tensor_by_name('image_tensor:0')
    boxes = mobilenet.get_tensor_by_name('detection_boxes:0')
    # probabilities
    scores = mobilenet.get_tensor_by_name('detection_scores:0')

    # prediction (detection)
    (boxes, scores) = sess.run([boxes, scores], feed_dict={image_tensor:img})

    # Readjust sizes of boxes and scores (we must remove a dimension)
    boxes = np.squeeze(boxes,axis=0)
    scores = np.squeeze(scores,axis=0)

    # Only bounding boxes that have a face
    idx = np.where(scores>=score_threshold)[0]

    # identify the coordinates of the box
    # create bounding boxes (we add all the ones that appear to us to a list)
    bboxes = []
    for index in idx:
        ymin, xmin, ymax, xmax = boxes[index,:]
        # change the coordinates so that open cv accepts them
        (left, right, top, bottom) = (xmin*imw, xmax*imw, ymin*imh, ymax*imh)
        # represent it as integers
        left, right, top, bottom = int(left), int(right), int(top), int(bottom)
        # store them in the bboxes variable
        bboxes.append([left,right,top,bottom])

    return bboxes

# Draw bounding boxes
def draw_box(image,box,color,line_width=6):
    if box==[]:
        return image
    else:
        cv2.rectangle(image,(box[0],box[2]),(box[1],box[3]),color,line_width)
    return image

name = 'reference_image.jpg'
image = load_image(DIR_UNKNOWNS,name)
bboxes = detect_faces(image)
for box in bboxes:
    detected_faces = draw_box(image,box,(0,255,0))
fig = plt.figure(figsize=(10,10))
plt.imshow(detected_faces)

# extract faces
def extract_faces(image,bboxes,new_size=(160,160)):
  # Store faces in a list
    cropped_faces = []
    for box in bboxes:
        left, right, top, bottom = box
       # coordinates from top to bottom and from left to right
        face = image[top:bottom,left:right]
        cropped_faces.append(cv2.resize(face,dsize=new_size))
    return cropped_faces

faces = extract_faces(image,bboxes)
plt.imshow(faces[2])

# Facenet
facenet = FaceNet()

# Generate 128 embeddings
def compute_embedding(model,face):
  # tenemos que pasar la imagen a tipo punto flotante
  # porque tenemos que normalizar la entrada
  face = face.astype('float32')
  #detections = model.extract(face,threshold=0.95)

  # nuestra image tiene que tener 4 dimensiones asi que le agregamos una
  face = np.expand_dims(face,axis=0)

  # predicci√≥n
  embedding = model.embeddings(face)
  return embedding

embedding = compute_embedding(facenet,faces[2])
print(embedding)

# calculate the reference embeddings (known faces)
known_embeddings = []

print('Processing known faces...')
for name in os.listdir(DIR_KNOWNS):
    if name.endswith('.jpg'):
        print(f'   {name}')
        image = load_image(DIR_KNOWNS,name)
        bboxes = detect_faces(image)
        face = extract_faces(image,bboxes)
        known_embeddings.append(compute_embedding(facenet,face[0]))

print(known_embeddings)

# function to compare faces
# calculate the distance between the 128 elements
def compare_faces(embs_ref, emb_desc, umbral=11):
    distancias = []
    for emb_ref in embs_ref:
        distancias.append(np.linalg.norm(emb_ref-emb_desc))
    distancias = np.array(distancias)
    return distancias, list(distancias<=umbral)

print('Processing unknown images...')
for name in os.listdir(DIR_UNKNOWNS):
    if name.endswith('.jpg'):
        print(f'   {name}')
        image = load_image(DIR_UNKNOWNS,name)
        bboxes = detect_faces(image)
        faces = extract_faces(image,bboxes)

        # For each face calculate embedding
        img_with_boxes = image.copy()
        for face, box in zip(faces,bboxes):
            emb = compute_embedding(facenet,face)

            _, recognition = compare_faces(known_embeddings,emb)

            if any(recognition):
                print('     match!')
                img_with_boxes = draw_box(img_with_boxes,box,(255,0,0))
            else:
                img_with_boxes = draw_box(img_with_boxes,box,(0,255,0))

        cv2.imwrite(f'{DIR_RESULTS}/{name}',cv2.cvtColor(img_with_boxes,cv2.COLOR_RGB2BGR))
print('End!')